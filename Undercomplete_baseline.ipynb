{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9556156e",
   "metadata": {},
   "source": [
    "### IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e63cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# autoencoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# deep learning\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    Dense,\n",
    "    Flatten,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow \n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models,layers,activations,losses,optimizers,metrics\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05e488",
   "metadata": {},
   "source": [
    "### LOADING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8504799a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/compat/_optional.py:126\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:984\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m credit_card \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcard transactions.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/excel/_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/excel/_base.py:1419\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[0;32m-> 1419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py:524\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     filepath_or_buffer: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[1;32m    512\u001b[0m     storage_options: StorageOptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;124;03m    Reader using openpyxl engine.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;124;03m        passed to fsspec for appropriate URLs (see ``_get_filepath_or_buffer``)\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 524\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(filepath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/compat/_optional.py:129\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl."
     ]
    }
   ],
   "source": [
    "credit_card = pd.read_excel(\"card transactions.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc5ca2",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f3e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit_card.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b408817",
   "metadata": {},
   "source": [
    "### EXTRACT DAY AND MONTH FOR FURTHER ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "day =pd.to_datetime(credit_card['Date']).dt.dayofweek\n",
    "month =pd.to_datetime(credit_card['Date']).dt.month\n",
    "credit_card.insert(1, 'day', day)\n",
    "credit_card.insert(2, 'month', month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022c42dd",
   "metadata": {},
   "source": [
    "### DELETING IRRELEVANT COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d006a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card = credit_card.drop('Date', axis = 1)\n",
    "credit_card = credit_card.drop('Recnum', axis = 1)\n",
    "credit_card = credit_card.drop('Transtype', axis = 1)\n",
    "credit_card = credit_card.drop('Merch zip', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c70308",
   "metadata": {},
   "source": [
    "### CHECKING FOR NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0f834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "credit_card.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140c6c6",
   "metadata": {},
   "source": [
    "### DROP THE NULL VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf38ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card['Merchnum'] = credit_card['Merchnum'].dropna()\n",
    "credit_card['Merch state'] = credit_card['Merch state'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2fa03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a4841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card[['day', 'month','Amount']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d63d68",
   "metadata": {},
   "source": [
    "### UNIQUE VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c400a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_unique = {}\n",
    "for i in credit_card.columns.to_list():\n",
    "    dict_unique[i]=len(credit_card[i].unique())\n",
    "dict_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df484be8",
   "metadata": {},
   "source": [
    "### CHECKING FOR DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b192349",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8491b40",
   "metadata": {},
   "source": [
    "### STATISTICS SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ca4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70a24b",
   "metadata": {},
   "source": [
    "### EXPLORING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5163db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalVar = credit_card.columns.tolist()\n",
    "for col in categoricalVar:\n",
    "    df = pd.DataFrame(credit_card[col].value_counts().sort_values(ascending=False).head(20))\n",
    "    df.plot(kind='bar',figsize=(12,6))\n",
    "    plt.xlabel(col,fontsize=15)\n",
    "    plt.ylabel('Frequency',fontsize=15)\n",
    "    plt.xticks(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccbaed7",
   "metadata": {},
   "source": [
    "### CORRELATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Pearson Correlation Matrix')\n",
    "sns.heatmap(credit_card.corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"winter\",\n",
    "            linecolor='w',annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4223a",
   "metadata": {},
   "source": [
    "The correlation plot shows no presence of multi collinearity amongst the features. All of these features does not show high correlation with the target class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d09423",
   "metadata": {},
   "source": [
    "### DISTRIBUTION OF TARGET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2507425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "labels = ['Not Fraud' , 'Frauds']\n",
    "explode = [.01,.01]\n",
    "color = ['skyblue' , 'Red']\n",
    "sizes = credit_card.Fraud.value_counts().values\n",
    "\n",
    "plt.pie(sizes,explode,labels,autopct=\"%1.1f%%\", colors = color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c7a279",
   "metadata": {},
   "source": [
    "The pie chart shows highly imbalanced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b15b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=sns.histplot(x='Amount',data=credit_card[credit_card.Amount<=1000],\n",
    "                hue='Fraud',stat='percent',multiple='dodge',common_norm=False,bins=25)\n",
    "ax.set_ylabel('Percentage in Each Type')\n",
    "ax.set_xlabel('Transaction Amount ')\n",
    "plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae892f1a",
   "metadata": {},
   "source": [
    "### STATE VS FRAUD : let's also explore which geographies are more prone to fraud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92790004",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=credit_card['Merch state'][credit_card.Fraud==0].value_counts(normalize=True)\n",
    "a=a.to_frame()\n",
    "a=a.reset_index()\n",
    "a.columns = ['Merch state', 'Per']\n",
    "\n",
    "b=credit_card['Merch state'][credit_card.Fraud==1].value_counts(normalize=True)\n",
    "b=b.to_frame()\n",
    "b=b.reset_index()\n",
    "b.columns = ['Merch state', 'Per']\n",
    "\n",
    "merged=a.merge(b,on='Merch state')\n",
    "merged['diff']=merged['Per_y']-merged['Per_x']\n",
    "merged['diff']=merged['diff']*100\n",
    "merged=merged.sort_values('diff',ascending=False)\n",
    "\n",
    "ax1=sns.barplot(data=merged, x='diff',y='Merch state')\n",
    "ax1.set_xlabel('Percentage Difference')\n",
    "ax1.set_ylabel('Merch state')\n",
    "plt.title('The Percentage of Fraudulent over Non-Fraudulent Transcations in Each State')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f51b0",
   "metadata": {},
   "source": [
    "### Monthly trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94a701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#month vs fraud\n",
    "ax=sns.histplot(data=credit_card, x=\"month\", hue=\"Fraud\", common_norm=False,stat='percent',multiple='dodge')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xlabel('Month')\n",
    "plt.xticks(np.arange(1,13,1))\n",
    "ax.set_xticklabels([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",'Aug','Sep','Oct','Nov','Dec'])\n",
    "plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f524fd",
   "metadata": {},
   "source": [
    "### Daily trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=sns.histplot(data = credit_card, x=\"day\", hue=\"Fraud\", common_norm=False,stat='percent',multiple='dodge')\n",
    "ax.set_xticklabels(['',\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"])\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.set_xlabel('Day')\n",
    "plt.legend(title='Type', labels=['Fraud', 'Not Fraud'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71f4ff",
   "metadata": {},
   "source": [
    "### ENCODING INDEPENDENT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "credit_card[\"Merchnum\"] = le.fit_transform(credit_card['Merchnum'])\n",
    "credit_card[\"Merch description\"] = le.fit_transform(credit_card['Merch description'])\n",
    "credit_card[\"Merch state\"] = le.fit_transform(credit_card['Merch state'])\n",
    "credit_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ef868",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a1e49",
   "metadata": {},
   "source": [
    "### FEATURE SCALING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00771989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale amount by log\n",
    "# Adding a small amount of 0.0001 to amount as log of zero is infinite.\n",
    "credit_card['amount_log'] = np.log(credit_card.Amount + 0.0001)\n",
    "credit_card['Cardnum_log'] = np.log(credit_card.Cardnum + 0.0001)\n",
    "credit_card['Merchnum_log'] = np.log(credit_card.Merchnum + 0.0001)\n",
    "credit_card['Merch description_log'] = np.log(credit_card['Merch description'] + 0.0001)\n",
    "credit_card['Merch state_log'] = np.log(credit_card['Merch state'] + 0.0001)\n",
    "credit_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e5fb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler() # object of the class StandardScaler ()\n",
    "credit_card['amount_scaled'] = ss.fit_transform(credit_card['Amount'].values.reshape(-1,1))\n",
    "credit_card['Cardnum_scaled'] = ss.fit_transform(credit_card['Cardnum'].values.reshape(-1,1))\n",
    "credit_card['Merchnum_scaled'] = ss.fit_transform(credit_card['Merchnum'].values.reshape(-1,1))\n",
    "credit_card['Merch description_scaled'] = ss.fit_transform(credit_card['Merch description'].values.reshape(-1,1))\n",
    "credit_card['Merch state_scaled'] = ss.fit_transform(credit_card['Merch state'].values.reshape(-1,1))\n",
    "\n",
    "credit_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ceb4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "mm = MinMaxScaler() # object of the class StandardScaler ()\n",
    "credit_card['amount_minmax'] = mm.fit_transform(credit_card['Amount'].values.reshape(-1,1))\n",
    "credit_card['Cardnum_minmax'] = mm.fit_transform(credit_card['Cardnum'].values.reshape(-1,1))\n",
    "credit_card['Merchnum_minmax'] = mm.fit_transform(credit_card['Merchnum'].values.reshape(-1,1))\n",
    "credit_card['Merch description_minmax'] = mm.fit_transform(credit_card['Merch description'].values.reshape(-1,1))\n",
    "credit_card['Merch state_minmax'] = mm.fit_transform(credit_card['Merch state'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering to a better visualization of the values\n",
    "\n",
    "# Let's explore the Amount by Class and see the distribuition of Amount transactions\n",
    "fig , axs = plt.subplots(nrows = 1 , ncols = 4 , figsize = (16,4))\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Amount\",data=credit_card, ax = axs[0])\n",
    "axs[0].set_title(\"Fraud vs Amount\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"amount_log\",data=credit_card, ax = axs[1])\n",
    "axs[1].set_title(\"Fraud vs Log Amount\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"amount_scaled\",data=credit_card, ax = axs[2])\n",
    "axs[2].set_title(\"Fraud vs Scaled Amount\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"amount_minmax\",data=credit_card, ax = axs[3])\n",
    "axs[3].set_title(\"Fraud vs Min Max Amount\")\n",
    "\n",
    "# fig.suptitle('Amount by Class', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axs = plt.subplots(nrows = 1 , ncols = 4 , figsize = (16,4))\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Cardnum\",data=credit_card, ax = axs[0])\n",
    "axs[0].set_title(\"Fraud vs Cardnum\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Cardnum_log\",data=credit_card, ax = axs[1])\n",
    "axs[1].set_title(\"Fraud vs Cardnum_log\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Cardnum_scaled\",data=credit_card, ax = axs[2])\n",
    "axs[2].set_title(\"Fraud vs Cardnum_scaled\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Cardnum_minmax\",data=credit_card, ax = axs[3])\n",
    "axs[3].set_title(\"Fraud vs Cardnum_minmax\")\n",
    "\n",
    "# fig.suptitle('Amount by Class', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b564b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , axs = plt.subplots(nrows = 1 , ncols = 4 , figsize = (16,4))\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Merchnum\",data=credit_card, ax = axs[0])\n",
    "axs[0].set_title(\"Fraud vs Amount\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Merchnum_log\",data=credit_card, ax = axs[1])\n",
    "axs[1].set_title(\"Class vs Log Amount\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Merchnum_scaled\",data=credit_card, ax = axs[2])\n",
    "axs[2].set_title(\"Class vs Scaled Amount\")\n",
    "\n",
    "sns.boxplot(x =\"Fraud\",y=\"Merchnum_minmax\",data=credit_card, ax = axs[3])\n",
    "axs[3].set_title(\"Class vs Min Max Amount\")\n",
    "\n",
    "# fig.suptitle('Amount by Class', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49b4fa0",
   "metadata": {},
   "source": [
    "We can see a slight difference in the log amount of our two Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = credit_card[['day', 'month', 'Cardnum_minmax','Merchnum_log','Merch description_log', 'Merch state_log', 'amount_log','Fraud']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be435481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "X_train = X_train[X_train.Fraud == 0]\n",
    "X_train = X_train.drop(['Fraud'], axis=1)\n",
    "\n",
    "y_test = X_test['Fraud']\n",
    "X_test = X_test.drop(['Fraud'], axis=1)\n",
    "\n",
    "X_train = X_train\n",
    "X_test = X_test\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d802e",
   "metadata": {},
   "source": [
    "### UNDERCOMPLETE AUTOENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a1f9ad",
   "metadata": {},
   "source": [
    "The Autoencoder model for anomaly detection has some steps as follows: \n",
    "\n",
    "Step 1 is the encoder step. The essential information is extracted by a neural network model in this step.\n",
    "\n",
    "Step 2 is the decoder step. In this step, the model reconstructs the data using the extracted information.\n",
    "\n",
    "Step 3: Iterate step 1 and step 2 to adjust the model to minimize the difference between input and reconstructed output, until we get good reconstruction results for the training dataset.\n",
    "\n",
    "Step 4: Make predictions on a dataset that includes outliers.\n",
    "\n",
    "Step 5: Set up a threshold for outliers/anomalies by comparing the differences between the autoencoder model reconstruction value and the actual value.\n",
    "\n",
    "Step 6: Identify the data points with the difference higher than the threshold to be outliers or anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc317d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tanh' is used for the activation function of the encoded network \n",
    "# because it has big learning steps and results in strong or higher gradients\n",
    "\n",
    "#relu is used on the bottle neck (latent low representation) \n",
    "#because model performance is better when trained with relu\n",
    "\n",
    "#sigmoid is used for binary classification and since we have \n",
    "#fraud and non fraudclass then it is good for this problem\n",
    "\n",
    "#In the input layer, we specified the shape of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67eb539",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = (X_train.shape[1])\n",
    "encoder = models.Sequential(name='encoder')\n",
    "encoder.add(layer=layers.Dense(units=200, activation=activations.tanh, activity_regularizer=regularizers.l1(1e-3),input_shape=[n_features]))\n",
    "encoder.add(layer=layers.Dense(units=100, activation=activations.tanh))\n",
    "encoder.add(layer=layers.Dense(units=5, activation=activations.relu))\n",
    "\n",
    "decoder = models.Sequential(name='decoder')\n",
    "decoder.add(layer=layers.Dense(units=100, activation=activations.tanh, input_shape=[5]))\n",
    "decoder.add(layer=layers.Dense(units=200, activation=activations.tanh))\n",
    "decoder.add(layer=layers.Dense(units=n_features, activation=activations.sigmoid))\n",
    "\n",
    "autoencoder = models.Sequential([encoder, decoder])\n",
    "\n",
    "autoencoder.compile(loss=losses.MSE,optimizer=optimizers.Adam(),metrics=[metrics.mean_squared_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the auto encoder model  with the optimizer of adam and the loss of mse (Mean squared Error).\n",
    "#autoencoder.compile(optimizer=\"adam\", loss=\"mse\", metrics=['AUC'])\n",
    "\n",
    "#When fitting the autoencoder model, we can see that the input and output datasets are the same,\n",
    "#which is the dataset that contains only the normal data points.\n",
    "\n",
    "#The epochs of 50 and batch_size of 32 mean the model uses 32 datapoints to update the weights in each iteration, \n",
    "#and the model will go through the whole training dataset 50 times.\n",
    "\n",
    "#shuffle=True will shuffle the dataset before each epoch.\n",
    "\n",
    "# training the auto encoder model on non fraud data \n",
    "history = autoencoder.fit(X_train, X_train,batch_size=128,epochs=10,shuffle=True,validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "\n",
    "plt.plot(history.history[\"loss\"], \"black\", linewidth=2.0)\n",
    "plt.plot(history.history[\"val_loss\"], \"green\", linewidth=2.0)\n",
    "plt.legend([\"Training Loss\", \"Validation Loss\"], fontsize=14)\n",
    "plt.xlabel(\"Epochs\", fontsize=10)\n",
    "plt.ylabel(\"Loss\", fontsize=10)\n",
    "#plt.ylim([0,1])\n",
    "plt.title(\"Loss Curves\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62024657",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have an autoencoder model, let's use it to predict the outliers.\n",
    "#Firstly, we use .predict to get the reconstruction value for the testing data \n",
    "#set containing the usual data points and the outliers.\n",
    "\n",
    "\n",
    "# Predict anomalies/outliers in the training dataset\n",
    "prediction = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328b37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean(np.power(X_test - prediction, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f4c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8491e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean squared error between actual and reconstruction/prediction\n",
    "prediction_loss = tf.keras.losses.mse(prediction, X_test)\n",
    "prediction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a116b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction loss threshold for 50% of outliers\n",
    "loss_threshold = np.percentile(prediction_loss, 50)\n",
    "print(f'The prediction loss threshold for 50% of outliers is {loss_threshold:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction error with fraud\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fraud_error_df = error_df[error_df['true_class'] == 1.0]\n",
    "_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7218ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.001, 1])\n",
    "plt.ylim([0, 1.001])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bd1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall,th= precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2950b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n",
    "plt.title('Precision for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n",
    "plt.title('Recall for different threshold values')\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65767a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 27.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa7dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17efce9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame({'true': error_df.true_class,\n",
    "                           'predicted': pred_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a12500",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(error_df.true_class, pred_y)\n",
    "conf_matrix\n",
    "plt.figure(figsize=(12, 12))\n",
    "LABELS = ['Normal', 'Fraud']\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b062b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the prediction performance\n",
    "print(classification_report(y_test, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0d9589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
